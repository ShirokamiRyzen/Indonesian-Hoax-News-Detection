{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ed23f7",
   "metadata": {},
   "source": [
    "# Hoax Detection System - Complete Notebook\n",
    "\n",
    "This notebook contains a complete hoax detection system that includes:\n",
    "1. **Data Scraping** - Collecting news articles from various sources\n",
    "2. **Data Preprocessing** - Cleaning and normalizing text data\n",
    "3. **Model Training** - Training machine learning models for hoax detection\n",
    "4. **Prediction** - Using trained models to predict whether news is hoax or valid\n",
    "\n",
    "## Overview\n",
    "- **Sources**: Kompas.com (valid news) and TurnBackHoax.id (hoax articles)\n",
    "- **Models**: Naive Bayes and Random Forest classifiers\n",
    "- **Features**: TF-IDF vectorization with text preprocessing\n",
    "- **Language**: Indonesian text processing using Sastrawi and NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5ea8a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our hoax detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping and data handling\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "from typing import List, Dict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Natural Language Processing\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd3e69",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants\n",
    "\n",
    "Define constants and configuration for our scraping and processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping configuration\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                   \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36\")\n",
    "}\n",
    "\n",
    "# Kompas scraping config\n",
    "INDEX_URL = \"https://indeks.kompas.com/\"\n",
    "PAGE_URL = INDEX_URL + \"?page={}\"\n",
    "MAX_PAGES = 10  # Reduced for demo purposes\n",
    "\n",
    "# TurnBackHoax scraping config\n",
    "TURNBACK_BASE_URL = \"https://turnbackhoax.id/page/\"\n",
    "TURNBACK_MAX_PAGES = 10  # Reduced for demo purposes\n",
    "\n",
    "# Classification tags\n",
    "HOAX_TAGS = (\"[SALAH]\", \"[PENIPUAN]\", \"[FITNAH]\", \"[DISINFORMASI]\", \"[HOAX]\")\n",
    "NON_HOAX_TAGS = (\"[VALID]\", \"[BENAR]\", \"[FAKTA]\", \"[KLARIFIKASI]\")\n",
    "\n",
    "# Directory paths\n",
    "DATA_DIR = \"data\"\n",
    "CLEAN_DATA_DIR = \"data_clean\"\n",
    "MODEL_DIR = \"model\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, CLEAN_DATA_DIR, MODEL_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(\"Configuration set up successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746833cb",
   "metadata": {},
   "source": [
    "## 3. Data Scraping Functions\n",
    "\n",
    "### 3.1 Kompas.com Scraper (Valid News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_kompas_index(pages: int = MAX_PAGES) -> List[Dict]:\n",
    "    \"\"\"Scrape news articles from Kompas.com index pages\"\"\"\n",
    "    records: List[Dict] = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = INDEX_URL if page == 1 else PAGE_URL.format(page)\n",
    "        print(f\"Scraping Kompas page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            html = requests.get(url, headers=HEADERS, timeout=30).text\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Try different article selectors\n",
    "        items = soup.select(\"div.article__list article\")\n",
    "        if not items:\n",
    "            items = soup.select(\"div.articleList.-list div.articleItem\")\n",
    "\n",
    "        for art in items:\n",
    "            title_tag = art.select_one(\"h3.article__title a, h2.articleTitle\")\n",
    "            if not title_tag:\n",
    "                continue\n",
    "                \n",
    "            link = title_tag[\"href\"] if title_tag.name == \"a\" else art.select_one(\"a.article-link\")[\"href\"]\n",
    "            title = title_tag.get_text(strip=True)\n",
    "\n",
    "            # Add [VALID] tag to mark as legitimate news\n",
    "            title = \"[VALID] \" + title\n",
    "\n",
    "            # Extract date and author if available\n",
    "            date_tag = art.select_one(\"div.article__date, div.articlePost-date\")\n",
    "            author_tag = art.select_one(\"div.article__author\")\n",
    "            tanggal = date_tag.get_text(strip=True) if date_tag else \"\"\n",
    "            author = author_tag.get_text(strip=True) if author_tag else \"\"\n",
    "\n",
    "            excerpt = get_article_excerpt(link)\n",
    "            records.append({\n",
    "                \"Judul\": title,\n",
    "                \"Link\": link,\n",
    "                \"Tanggal\": tanggal,\n",
    "                \"Author\": author,\n",
    "                \"Isi Ringkas\": excerpt\n",
    "            })\n",
    "    \n",
    "    return records\n",
    "\n",
    "def get_article_excerpt(url: str) -> str:\n",
    "    \"\"\"Extract article excerpt from the given URL\"\"\"\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        p = soup.select_one(\"div.read__content p\")\n",
    "        return p.get_text(strip=True) if p else \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "print(\"Kompas scraper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1af3c",
   "metadata": {},
   "source": [
    "### 3.2 TurnBackHoax.id Scraper (Hoax Articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_turnbackhoax(max_pages: int = TURNBACK_MAX_PAGES) -> List[Dict]:\n",
    "    \"\"\"Scrape hoax articles from TurnBackHoax.id\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{TURNBACK_BASE_URL}{page}/\"\n",
    "        print(f\"Scraping TurnBackHoax page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            res.raise_for_status()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        articles = soup.select(\"article.mh-loop-item\")\n",
    "\n",
    "        for article in articles:\n",
    "            title_tag = article.select_one(\"h3.entry-title a\")\n",
    "            date_tag = article.select_one(\"span.mh-meta-date\")\n",
    "            author_tag = article.select_one(\"span.mh-meta-author a\")\n",
    "            excerpt_tag = article.select_one(\"div.mh-excerpt p\")\n",
    "\n",
    "            data.append({\n",
    "                \"Judul\": title_tag.text.strip() if title_tag else \"\",\n",
    "                \"Link\": title_tag[\"href\"] if title_tag else \"\",\n",
    "                \"Tanggal\": date_tag.text.strip() if date_tag else \"\",\n",
    "                \"Author\": author_tag.text.strip() if author_tag else \"\",\n",
    "                \"Isi Ringkas\": excerpt_tag.text.strip() if excerpt_tag else \"\"\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(\"TurnBackHoax scraper function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9345801a",
   "metadata": {},
   "source": [
    "### 3.3 Run Scrapers and Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data: List[Dict], filename: str, directory: str = DATA_DIR):\n",
    "    \"\"\"Save data to CSV file\"\"\"\n",
    "    path = os.path.join(directory, filename)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Data saved to {path} ({len(df)} rows)\")\n",
    "    return df\n",
    "\n",
    "# Scrape Kompas data (valid news)\n",
    "print(\"=== Scraping Kompas.com (Valid News) ===\")\n",
    "kompas_data = scrape_kompas_index(pages=3)  # Reduced for demo\n",
    "kompas_df = save_to_csv(kompas_data, \"kompas.csv\")\n",
    "\n",
    "print(\"\\n=== Scraping TurnBackHoax.id (Hoax Articles) ===\")\n",
    "turnback_data = scrape_turnbackhoax(max_pages=3)  # Reduced for demo\n",
    "turnback_df = save_to_csv(turnback_data, \"turnbackhoax.csv\")\n",
    "\n",
    "print(\"\\n=== Scraping Complete ===\")\n",
    "print(f\"Kompas articles: {len(kompas_data)}\")\n",
    "print(f\"TurnBackHoax articles: {len(turnback_data)}\")\n",
    "print(f\"Total articles: {len(kompas_data) + len(turnback_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1636156",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Text Normalization\n",
    "\n",
    "### 4.1 Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bd844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and stopwords\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize Indonesian text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [w for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    \n",
    "    # Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def process_batch(df_chunk):\n",
    "    \"\"\"Process a batch of data for parallel processing\"\"\"\n",
    "    df_chunk[\"Isi Ringkas Clean\"] = df_chunk[\"Isi Ringkas\"].apply(clean_text)\n",
    "    return df_chunk\n",
    "\n",
    "print(\"Text cleaning functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c57e0",
   "metadata": {},
   "source": [
    "### 4.2 Process and Clean All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf4d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_files():\n",
    "    \"\"\"Process all CSV files in the data directory\"\"\"\n",
    "    all_files = [f for f in os.listdir(DATA_DIR) if f.endswith(\".csv\")]\n",
    "    \n",
    "    for filename in all_files:\n",
    "        print(f\"\\nProcessing {filename}...\")\n",
    "        df = pd.read_csv(os.path.join(DATA_DIR, filename))\n",
    "        \n",
    "        # Check if required column exists\n",
    "        if \"Isi Ringkas\" not in df.columns:\n",
    "            print(f\"Column 'Isi Ringkas' not found in {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Clean the text data\n",
    "        print(f\"Cleaning text data for {len(df)} articles...\")\n",
    "        df[\"Isi Ringkas Clean\"] = df[\"Isi Ringkas\"].apply(clean_text)\n",
    "        \n",
    "        # Save cleaned data\n",
    "        out_path = os.path.join(CLEAN_DATA_DIR, filename.replace(\".csv\", \"_cleaned.csv\"))\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Cleaned data saved to: {out_path}\")\n",
    "        \n",
    "        # Show sample of cleaned data\n",
    "        if len(df) > 0:\n",
    "            print(f\"Sample cleaned text: {df['Isi Ringkas Clean'].iloc[0][:100]}...\")\n",
    "\n",
    "# Process all data files\n",
    "process_data_files()\n",
    "print(\"\\n=== Data preprocessing complete! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991ac7d",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "### 5.1 Data Preparation and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c75317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label(title: str) -> int:\n",
    "    \"\"\"Create label based on article title tags\"\"\"\n",
    "    title = title.upper()\n",
    "    \n",
    "    # Check for HOAX tags\n",
    "    if any(tag in title for tag in HOAX_TAGS):\n",
    "        return 1  # HOAX\n",
    "    \n",
    "    # Check for VALID tags\n",
    "    if any(tag in title for tag in NON_HOAX_TAGS):\n",
    "        return 0  # VALID\n",
    "    \n",
    "    # For Kompas articles without explicit tags, assume valid\n",
    "    if \"kompas\" in title.lower():\n",
    "        return 0  # VALID\n",
    "    \n",
    "    # Default to HOAX for TurnBackHoax articles\n",
    "    return 1  # HOAX\n",
    "\n",
    "def load_training_data(clean_dir: str = CLEAN_DATA_DIR):\n",
    "    \"\"\"Load and prepare training data\"\"\"\n",
    "    # Load all cleaned CSV files\n",
    "    frames = []\n",
    "    for filename in os.listdir(clean_dir):\n",
    "        if filename.endswith(\"_cleaned.csv\"):\n",
    "            df = pd.read_csv(os.path.join(clean_dir, filename))\n",
    "            frames.append(df)\n",
    "    \n",
    "    if not frames:\n",
    "        raise FileNotFoundError(\"No cleaned CSV files found in data_clean/\")\n",
    "    \n",
    "    # Combine all data\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    # Create labels\n",
    "    df[\"label\"] = df[\"Judul\"].apply(make_label)\n",
    "    \n",
    "    # Remove rows with missing text or labels\n",
    "    df = df.dropna(subset=[\"Isi Ringkas Clean\", \"label\"])\n",
    "    df = df[df[\"Isi Ringkas Clean\"].str.len() > 0]\n",
    "    \n",
    "    texts = df[\"Isi Ringkas Clean\"]\n",
    "    labels = df[\"label\"].astype(int)\n",
    "    \n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(f\"Total articles: {len(df)}\")\n",
    "    print(\"Label distribution:\")\n",
    "    label_counts = labels.value_counts().rename({0: \"VALID\", 1: \"HOAX\"})\n",
    "    print(label_counts)\n",
    "    print(f\"Class balance: {label_counts.min() / label_counts.max():.2f}\")\n",
    "    \n",
    "    return texts, labels, df\n",
    "\n",
    "# Load training data\n",
    "texts, labels, full_df = load_training_data()\n",
    "print(\"\\nTraining data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772f74c",
   "metadata": {},
   "source": [
    "### 5.2 Feature Extraction and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(texts, labels, ngram_range=(1, 2)):\n",
    "    \"\"\"Train Naive Bayes and Random Forest models\"\"\"\n",
    "    print(f\"\\n=== Training Models ===\")\n",
    "    print(f\"TF-IDF n-gram range: {ngram_range}\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,\n",
    "        ngram_range=ngram_range,\n",
    "        sublinear_tf=True,\n",
    "        stop_words=None  # We already removed stopwords\n",
    "    )\n",
    "    \n",
    "    # Vectorize the text\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Train Naive Bayes\n",
    "    print(\"\\nTraining Naive Bayes...\")\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,  # Reduced for faster training\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    print(\"\\n=== Model Evaluation ===\")\n",
    "    \n",
    "    for name, model in [(\"Naive Bayes\", nb_model), (\"Random Forest\", rf_model)]:\n",
    "        print(f\"\\n{name} Results:\")\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.3f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=[\"VALID\", \"HOAX\"], zero_division=0))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return vectorizer, nb_model, rf_model\n",
    "\n",
    "# Train the models\n",
    "vectorizer, nb_model, rf_model = train_models(texts, labels)\n",
    "print(\"\\n=== Model training complete! ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badfea0",
   "metadata": {},
   "source": [
    "### 5.3 Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(vectorizer, nb_model, rf_model, model_dir: str = MODEL_DIR):\n",
    "    \"\"\"Save trained models and vectorizer\"\"\"\n",
    "    print(f\"\\n=== Saving Models to {model_dir}/ ===\")\n",
    "    \n",
    "    # Save vectorizer\n",
    "    vectorizer_path = os.path.join(model_dir, \"vectorizer.pkl\")\n",
    "    joblib.dump(vectorizer, vectorizer_path)\n",
    "    print(f\"Vectorizer saved: {vectorizer_path}\")\n",
    "    \n",
    "    # Save Naive Bayes model\n",
    "    nb_path = os.path.join(model_dir, \"nb.pkl\")\n",
    "    joblib.dump(nb_model, nb_path)\n",
    "    print(f\"Naive Bayes model saved: {nb_path}\")\n",
    "    \n",
    "    # Save Random Forest model\n",
    "    rf_path = os.path.join(model_dir, \"rf.pkl\")\n",
    "    joblib.dump(rf_model, rf_path)\n",
    "    print(f\"Random Forest model saved: {rf_path}\")\n",
    "    \n",
    "    print(\"\\nAll models saved successfully!\")\n",
    "\n",
    "# Save the trained models\n",
    "save_models(vectorizer, nb_model, rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9f631",
   "metadata": {},
   "source": [
    "## 6. Prediction System\n",
    "\n",
    "### 6.1 Load Trained Models and Setup Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00829acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_models(model_dir: str = MODEL_DIR):\n",
    "    \"\"\"Load trained models for prediction\"\"\"\n",
    "    vectorizer_path = os.path.join(model_dir, \"vectorizer.pkl\")\n",
    "    nb_path = os.path.join(model_dir, \"nb.pkl\")\n",
    "    rf_path = os.path.join(model_dir, \"rf.pkl\")\n",
    "    \n",
    "    # Check if all model files exist\n",
    "    missing_files = []\n",
    "    for path in [vectorizer_path, nb_path, rf_path]:\n",
    "        if not os.path.exists(path):\n",
    "            missing_files.append(path)\n",
    "    \n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Missing model files: {missing_files}\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading trained models...\")\n",
    "    loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "    loaded_nb = joblib.load(nb_path)\n",
    "    loaded_rf = joblib.load(rf_path)\n",
    "    \n",
    "    print(\"Models loaded successfully!\")\n",
    "    return loaded_vectorizer, loaded_nb, loaded_rf\n",
    "\n",
    "# Load the trained models\n",
    "pred_vectorizer, pred_nb, pred_rf = load_trained_models()\n",
    "\n",
    "# Label mapping\n",
    "LABEL_MAP = {0: \"TIDAK HOAX\", 1: \"HOAX\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4ea33",
   "metadata": {},
   "source": [
    "### 6.2 Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hoax(text: str, show_details: bool = True):\n",
    "    \"\"\"Predict whether a text is hoax or not\"\"\"\n",
    "    # Clean the input text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    if not cleaned_text.strip():\n",
    "        return \"Error: Text is empty after cleaning\"\n",
    "    \n",
    "    # Vectorize the text\n",
    "    X = pred_vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Get predictions from both models\n",
    "    nb_prob = pred_nb.predict_proba(X)[0]\n",
    "    rf_prob = pred_rf.predict_proba(X)[0]\n",
    "    \n",
    "    # Probability of being HOAX (class 1)\n",
    "    nb_hoax_prob = nb_prob[1] * 100\n",
    "    rf_hoax_prob = rf_prob[1] * 100\n",
    "    \n",
    "    # Get predictions\n",
    "    nb_prediction = LABEL_MAP[pred_nb.predict(X)[0]]\n",
    "    rf_prediction = LABEL_MAP[pred_rf.predict(X)[0]]\n",
    "    \n",
    "    # Ensemble prediction (average probability)\n",
    "    avg_hoax_prob = (nb_hoax_prob + rf_hoax_prob) / 2\n",
    "    ensemble_prediction = \"HOAX\" if avg_hoax_prob >= 50 else \"TIDAK HOAX\"\n",
    "    \n",
    "    results = {\n",
    "        \"text\": text[:100] + \"...\" if len(text) > 100 else text,\n",
    "        \"cleaned_text\": cleaned_text[:100] + \"...\" if len(cleaned_text) > 100 else cleaned_text,\n",
    "        \"naive_bayes\": {\n",
    "            \"prediction\": nb_prediction,\n",
    "            \"hoax_probability\": round(nb_hoax_prob, 2)\n",
    "        },\n",
    "        \"random_forest\": {\n",
    "            \"prediction\": rf_prediction,\n",
    "            \"hoax_probability\": round(rf_hoax_prob, 2)\n",
    "        },\n",
    "        \"ensemble\": {\n",
    "            \"prediction\": ensemble_prediction,\n",
    "            \"hoax_probability\": round(avg_hoax_prob, 2)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"\\n=== Hoax Detection Results ===\")\n",
    "        print(f\"Text: {results['text']}\")\n",
    "        print(f\"\\nNaive Bayes: {results['naive_bayes']['prediction']} (HOAX prob: {results['naive_bayes']['hoax_probability']}%)\")\n",
    "        print(f\"Random Forest: {results['random_forest']['prediction']} (HOAX prob: {results['random_forest']['hoax_probability']}%)\")\n",
    "        print(f\"\\nü§ñ Final Prediction: {results['ensemble']['prediction']} (HOAX prob: {results['ensemble']['hoax_probability']}%)\")\n",
    "        \n",
    "        # Confidence level\n",
    "        confidence = abs(results['ensemble']['hoax_probability'] - 50)\n",
    "        if confidence > 30:\n",
    "            print(f\"Confidence: HIGH ({confidence:.1f}%)\")\n",
    "        elif confidence > 15:\n",
    "            print(f\"Confidence: MEDIUM ({confidence:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"Confidence: LOW ({confidence:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Prediction functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea3729",
   "metadata": {},
   "source": [
    "### 6.3 Test the Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c98656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample texts\n",
    "test_texts = [\n",
    "    \"Vaksin COVID-19 menyebabkan magnetisme pada tubuh manusia\",\n",
    "    \"Pemerintah mengumumkan program bantuan sosial untuk keluarga kurang mampu\",\n",
    "    \"Minum air putih hangat dapat menyembuhkan kanker dalam 3 hari\",\n",
    "    \"Bank Indonesia mengumumkan kebijakan suku bunga terbaru\",\n",
    "    \"Temuan ilmuwan: Makan nasi dapat menyebabkan kematian mendadak\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Prediction System ===\")\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    result = predict_hoax(text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5b5d3",
   "metadata": {},
   "source": [
    "## 7. Interactive Prediction Interface\n",
    "\n",
    "### 7.1 Batch Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(texts: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Predict multiple texts at once and return as DataFrame\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            result = predict_hoax(text, show_details=False)\n",
    "            results.append({\n",
    "                \"Text\": text[:50] + \"...\" if len(text) > 50 else text,\n",
    "                \"NB_Prediction\": result['naive_bayes']['prediction'],\n",
    "                \"NB_Prob\": result['naive_bayes']['hoax_probability'],\n",
    "                \"RF_Prediction\": result['random_forest']['prediction'],\n",
    "                \"RF_Prob\": result['random_forest']['hoax_probability'],\n",
    "                \"Final_Prediction\": result['ensemble']['prediction'],\n",
    "                \"Final_Prob\": result['ensemble']['hoax_probability']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"Text\": text[:50] + \"...\" if len(text) > 50 else text,\n",
    "                \"NB_Prediction\": \"Error\",\n",
    "                \"NB_Prob\": 0,\n",
    "                \"RF_Prediction\": \"Error\",\n",
    "                \"RF_Prob\": 0,\n",
    "                \"Final_Prediction\": \"Error\",\n",
    "                \"Final_Prob\": 0\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Test batch prediction\n",
    "batch_results = predict_batch(test_texts)\n",
    "print(\"\\n=== Batch Prediction Results ===\")\n",
    "print(batch_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504cf775",
   "metadata": {},
   "source": [
    "### 7.2 Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ad7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance():\n",
    "    \"\"\"Analyze model performance on the training dataset\"\"\"\n",
    "    print(\"=== Model Performance Analysis ===\")\n",
    "    \n",
    "    # Load cleaned data for analysis\n",
    "    texts, labels, df = load_training_data()\n",
    "    \n",
    "    # Get predictions for all data\n",
    "    X = pred_vectorizer.transform(texts)\n",
    "    \n",
    "    nb_predictions = pred_nb.predict(X)\n",
    "    rf_predictions = pred_rf.predict(X)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    nb_accuracy = accuracy_score(labels, nb_predictions)\n",
    "    rf_accuracy = accuracy_score(labels, rf_predictions)\n",
    "    \n",
    "    print(f\"\\nOverall Performance on Training Data:\")\n",
    "    print(f\"Naive Bayes Accuracy: {nb_accuracy:.3f}\")\n",
    "    print(f\"Random Forest Accuracy: {rf_accuracy:.3f}\")\n",
    "    \n",
    "    # Feature importance for Random Forest\n",
    "    if hasattr(pred_rf, 'feature_importances_'):\n",
    "        feature_names = pred_vectorizer.get_feature_names_out()\n",
    "        importance_scores = pred_rf.feature_importances_\n",
    "        \n",
    "        # Get top 10 most important features\n",
    "        top_indices = importance_scores.argsort()[-10:][::-1]\n",
    "        \n",
    "        print(f\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "        for i, idx in enumerate(top_indices, 1):\n",
    "            print(f\"{i:2d}. {feature_names[idx]:15s} (importance: {importance_scores[idx]:.4f})\")\n",
    "    \n",
    "    return nb_accuracy, rf_accuracy\n",
    "\n",
    "# Analyze performance\n",
    "nb_acc, rf_acc = analyze_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca89cca",
   "metadata": {},
   "source": [
    "## 8. Interactive Prediction Interface\n",
    "\n",
    "### 8.1 Custom Text Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc37dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_prediction():\n",
    "    \"\"\"Interactive function for testing custom texts\"\"\"\n",
    "    print(\"\\n=== Interactive Hoax Detection ===\")\n",
    "    print(\"Enter news text to check if it's potentially hoax or not.\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"Enter news text: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"Please enter some text.\")\n",
    "                continue\n",
    "            \n",
    "            # Predict\n",
    "            result = predict_hoax(user_input)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Uncomment the line below to run interactive mode\n",
    "# interactive_prediction()\n",
    "\n",
    "print(\"Interactive prediction function is ready!\")\n",
    "print(\"Uncomment the line above to run interactive mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c5604",
   "metadata": {},
   "source": [
    "### 8.2 Real-time News Article Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze recent articles from our dataset\n",
    "def analyze_sample_articles(n_samples: int = 5):\n",
    "    \"\"\"Analyze a sample of articles from the dataset\"\"\"\n",
    "    try:\n",
    "        # Load some sample data\n",
    "        sample_files = []\n",
    "        for filename in os.listdir(DATA_DIR):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                sample_files.append(filename)\n",
    "        \n",
    "        if not sample_files:\n",
    "            print(\"No data files found for analysis.\")\n",
    "            return\n",
    "        \n",
    "        # Load a sample file\n",
    "        sample_df = pd.read_csv(os.path.join(DATA_DIR, sample_files[0]))\n",
    "        \n",
    "        # Get random samples\n",
    "        if len(sample_df) > n_samples:\n",
    "            samples = sample_df.sample(n=n_samples)\n",
    "        else:\n",
    "            samples = sample_df\n",
    "        \n",
    "        print(f\"\\n=== Analyzing {len(samples)} Sample Articles ===\")\n",
    "        \n",
    "        for idx, row in samples.iterrows():\n",
    "            title = row.get('Judul', 'No Title')\n",
    "            content = row.get('Isi Ringkas', 'No Content')\n",
    "            \n",
    "            if pd.isna(content) or not content.strip():\n",
    "                print(f\"\\nArticle {idx}: {title[:60]}...\")\n",
    "                print(\"No content available for analysis.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n--- Article {idx} ---\")\n",
    "            print(f\"Title: {title}\")\n",
    "            result = predict_hoax(content)\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing articles: {e}\")\n",
    "\n",
    "# Analyze sample articles\n",
    "analyze_sample_articles(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a0d4b",
   "metadata": {},
   "source": [
    "## 9. Summary and Usage Guide\n",
    "\n",
    "### 9.1 System Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_system_summary():\n",
    "    \"\"\"Print a comprehensive summary of the hoax detection system\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"           HOAX DETECTION SYSTEM SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìä DATA SOURCES:\")\n",
    "    print(\"   ‚Ä¢ Kompas.com (Valid news articles)\")\n",
    "    print(\"   ‚Ä¢ TurnBackHoax.id (Hoax articles)\")\n",
    "    \n",
    "    print(\"\\nüîß PREPROCESSING:\")\n",
    "    print(\"   ‚Ä¢ Text normalization (lowercase, remove punctuation)\")\n",
    "    print(\"   ‚Ä¢ Stopword removal (Indonesian)\")\n",
    "    print(\"   ‚Ä¢ Stemming (Sastrawi)\")\n",
    "    print(\"   ‚Ä¢ TF-IDF vectorization\")\n",
    "    \n",
    "    print(\"\\nü§ñ MODELS:\")\n",
    "    print(\"   ‚Ä¢ Naive Bayes Classifier\")\n",
    "    print(\"   ‚Ä¢ Random Forest Classifier\")\n",
    "    print(\"   ‚Ä¢ Ensemble prediction (average probability)\")\n",
    "    \n",
    "    print(\"\\nüìÅ FILES CREATED:\")\n",
    "    print(f\"   ‚Ä¢ {DATA_DIR}/kompas.csv - Raw Kompas articles\")\n",
    "    print(f\"   ‚Ä¢ {DATA_DIR}/turnbackhoax.csv - Raw TurnBackHoax articles\")\n",
    "    print(f\"   ‚Ä¢ {CLEAN_DATA_DIR}/*_cleaned.csv - Processed articles\")\n",
    "    print(f\"   ‚Ä¢ {MODEL_DIR}/vectorizer.pkl - TF-IDF vectorizer\")\n",
    "    print(f\"   ‚Ä¢ {MODEL_DIR}/nb.pkl - Naive Bayes model\")\n",
    "    print(f\"   ‚Ä¢ {MODEL_DIR}/rf.pkl - Random Forest model\")\n",
    "    \n",
    "    print(\"\\nüéØ USAGE:\")\n",
    "    print(\"   ‚Ä¢ Use predict_hoax(text) for single predictions\")\n",
    "    print(\"   ‚Ä¢ Use predict_batch(texts) for multiple predictions\")\n",
    "    print(\"   ‚Ä¢ Run interactive_prediction() for testing\")\n",
    "    \n",
    "    print(\"\\nüìà PERFORMANCE:\")\n",
    "    if 'nb_acc' in globals() and 'rf_acc' in globals():\n",
    "        print(f\"   ‚Ä¢ Naive Bayes Accuracy: {nb_acc:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Random Forest Accuracy: {rf_acc:.3f}\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ Performance metrics available after training\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"         System ready for hoax detection!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Print system summary\n",
    "print_system_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff2963d",
   "metadata": {},
   "source": [
    "### 9.2 Quick Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26824ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== QUICK USAGE EXAMPLES ===\")\n",
    "\n",
    "# Example 1: Single prediction\n",
    "print(\"\\n1. Single Text Prediction:\")\n",
    "example_text = \"Penelitian terbaru menunjukkan bahwa minum kopi dapat mencegah diabetes.\"\n",
    "print(f\"Text: {example_text}\")\n",
    "result = predict_hoax(example_text, show_details=False)\n",
    "print(f\"Prediction: {result['ensemble']['prediction']} (Confidence: {result['ensemble']['hoax_probability']}%)\")\n",
    "\n",
    "# Example 2: Batch prediction\n",
    "print(\"\\n2. Batch Prediction:\")\n",
    "batch_texts = [\n",
    "    \"Pemerintah mengumumkan kebijakan ekonomi baru\",\n",
    "    \"Minum air lemon dapat menyembuhkan semua penyakit\",\n",
    "    \"Universitas Indonesia membuka program beasiswa\"\n",
    "]\n",
    "\n",
    "batch_df = predict_batch(batch_texts)\n",
    "print(batch_df[['Text', 'Final_Prediction', 'Final_Prob']].to_string(index=False))\n",
    "\n",
    "print(\"\\n3. How to use interactively:\")\n",
    "print(\"   # Uncomment and run this line for interactive mode:\")\n",
    "print(\"   # interactive_prediction()\")\n",
    "\n",
    "print(\"\\n=== System is ready for use! ===\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
